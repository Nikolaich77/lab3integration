name: Speech Commands API

on:
  workflow_dispatch:
  pull_request:
    branches: [ main, master ]
  push:
    branches: [ main, master ]

# –©–æ–± –ø—É—à–∏—Ç–∏ –≤ GHCR, –ø–æ—Ç—Ä—ñ–±–Ω—ñ –ø—Ä–∞–≤–∞ –Ω–∞ –ø–∞–∫–µ—Ç–∏
permissions:
  contents: read
  packages: write

env:
  IMAGE_NAME: speech-commands-api
  REGISTRY: ghcr.io
  PY_VERSION: '3.11'
  TRAIN_TAG: ${{ github.sha }}-train
  INFER_TAG: ${{ github.sha }}

jobs:
  train:
    name: üèãÔ∏è Build trainer image and run training
    runs-on: ubuntu-latest
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: ‚öôÔ∏è Patch training for CI (epochs=1, batch=16)
        run: |
          sed -i 's/^EPOCHS\s*=\s*10/EPOCHS = 1/' speech_commands_train.py || true
          sed -i 's/^BATCH_SIZE\s*=\s*64/BATCH_SIZE = 16/' speech_commands_train.py || true

      - name: üìù Prepare Dockerfile for training (CPU)
        run: |
          mkdir -p docker
          cat > docker/Trainer.Dockerfile <<'EOF'
          # syntax=docker/dockerfile:1.4
          ARG PY_VERSION
          FROM python:${PY_VERSION}-slim
          ENV DEBIAN_FRONTEND=noninteractive \
              PIP_NO_CACHE_DIR=1 \
              PYTHONUNBUFFERED=1
          RUN apt-get update && apt-get install -y --no-install-recommends \
                bash libsndfile1 sox ffmpeg \
              && rm -rf /var/lib/apt/lists/*
          WORKDIR /app
          COPY requirements.txt ./
          RUN pip install --no-cache-dir --upgrade pip setuptools wheel && \
              pip install --no-cache-dir --extra-index-url https://download.pytorch.org/whl/cpu -r requirements.txt && \
              pip install --no-cache-dir soundfile==0.12.1
          COPY . .
          # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –æ–±—Ä–∞–∑ –ª–∏—à–µ –º—ñ—Å—Ç–∏—Ç—å –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ —Ç–∞ –∫–æ–¥; —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è –Ω–∞ –µ—Ç–∞–ø—ñ `docker run`
          CMD ["bash","-lc","python -u speech_commands_train.py | tee train.log"]
          EOF

      - name: üîß Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: üèóÔ∏è Build trainer image (no push)
        uses: docker/build-push-action@v6
        with:
          context: .
          file: docker/Trainer.Dockerfile
          tags: ${{ env.IMAGE_NAME }}-train:${{ env.TRAIN_TAG }}
          load: true
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            PY_VERSION=${{ env.PY_VERSION }}

      - name: üöÄ Run training container and collect artifacts
        id: run-train
        run: |
          CID=train-${{ github.run_id }}
          docker run -d --name $CID ${{ env.IMAGE_NAME }}-train:${{ env.TRAIN_TAG }}
          # stream logs while running
          docker logs -f $CID &
          EXIT_CODE=$(docker wait $CID)
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          mkdir -p artifacts
          docker cp $CID:/app/model_state_dict.pt artifacts/ || true
          docker cp $CID:/app/model_scripted.pt artifacts/ || true
          docker cp $CID:/app/train.log artifacts/ || true
          docker rm -f $CID || true
          ls -lah artifacts || true

      - name: ‚ùå Fail if training failed
        if: steps.run-train.outputs.exit_code != '0'
        run: exit 1

      - name: üì¶ Upload training artifacts (model, logs)
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: |
            artifacts/model_state_dict.pt
            artifacts/model_scripted.pt
            artifacts/train.log
          if-no-files-found: warn

  verify-model:
    name: ‚úÖ Verify model integrity (sanity check)
    runs-on: ubuntu-latest
    needs: train
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üì• Download model artifacts
        uses: actions/download-artifact@v4
        with:
          name: model-artifacts
          path: artifacts

      - name: üêç Set up Python ${{ env.PY_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PY_VERSION }}

      - name: üìö Install minimal deps (CPU)
        run: |
          python -m pip install --upgrade pip
          pip install --extra-index-url https://download.pytorch.org/whl/cpu torch==2.1.0+cpu torchaudio==2.1.0+cpu soundfile==0.12.1

      - name: üß™ Run quick forward pass and write verification.json
        run: |
          python - <<'PY'
          import json, torch
          from pathlib import Path
          from model_utils import SmallCNN
          model_path = Path('artifacts/model_state_dict.pt')
          assert model_path.exists(), 'model_state_dict.pt missing'
          device = torch.device('cpu')
          model = SmallCNN(n_classes=4)
          model.load_state_dict(torch.load(model_path, map_location=device))
          model.eval()
          x = torch.randn(1,1,64,32)
          with torch.no_grad():
            y = model(x)
          ok = y.shape == (1,4)
          Path('artifacts').mkdir(exist_ok=True)
          Path('artifacts/verification.json').write_text(json.dumps({
            'ok': bool(ok),
            'output_shape': list(y.shape),
          }, indent=2))
          print('Verification OK:', ok)
          PY

      - name: üì¶ Upload verification artifact
        uses: actions/upload-artifact@v4
        with:
          name: verification
          path: artifacts/verification.json

  build-inference:
    name: üê≥ Build and optionally push inference image
    runs-on: ubuntu-latest
    needs: [train]
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üì• Download model artifacts
        uses: actions/download-artifact@v4
        with:
          name: model-artifacts
          path: artifacts

      - name: üìù Create inference Dockerfile (CPU)
        run: |
          mkdir -p docker
          cat > docker/Inference.Dockerfile <<'EOF'
          # syntax=docker/dockerfile:1.4
          ARG PY_VERSION
          FROM python:${PY_VERSION}-slim
          ENV DEBIAN_FRONTEND=noninteractive \
              PYTHONUNBUFFERED=1 \
              PIP_NO_CACHE_DIR=1
          RUN apt-get update && apt-get install -y --no-install-recommends \
                bash libsndfile1 sox ffmpeg \
              && rm -rf /var/lib/apt/lists/*
          WORKDIR /app
          COPY requirements.txt ./requirements.txt
          RUN pip install --no-cache-dir --upgrade pip setuptools wheel && \
              pip install --no-cache-dir --extra-index-url https://download.pytorch.org/whl/cpu -r requirements.txt && \
              pip install --no-cache-dir soundfile==0.12.1
          COPY app.py model_utils.py ./
          COPY templates/ ./templates/
          # –ö–æ–ø—ñ—é—î–º–æ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å —É –æ–±—Ä–∞–∑
          COPY artifacts/model_state_dict.pt ./model_state_dict.pt
          # –ó–∞–ø—É—Å–∫ Flask API
          EXPOSE 8000
          CMD ["python", "app.py"]
          EOF

      - name: üîß Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: üîê Log in to GHCR
        if: ${{ github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') }}
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: üèóÔ∏è Build inference image
        id: build-infer
        uses: docker/build-push-action@v6
        with:
          context: .
          file: docker/Inference.Dockerfile
          push: ${{ github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') }}
          tags: |
            ${{ env.REGISTRY }}/${{ github.repository }}:${{ env.INFER_TAG }}
            ${{ env.REGISTRY }}/${{ github.repository }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            PY_VERSION=${{ env.PY_VERSION }}

      - name: üíæ Save image as tar (artifact) when not pushing
        if: ${{ !(github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')) }}
        run: |
          docker save ${{ env.REGISTRY }}/${{ github.repository }}:${{ env.INFER_TAG }} -o inference-image.tar

      - name: üì¶ Upload inference image artifact (tar)
        if: ${{ !(github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')) }}
        uses: actions/upload-artifact@v4
        with:
          name: inference-image-tar
          path: inference-image.tar

  measure-latency:
    name: ‚ö° Measure latency and upload metrics
    runs-on: ubuntu-latest
    needs: [build-inference]
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üì• Download model artifacts (for reference)
        uses: actions/download-artifact@v4
        with:
          name: model-artifacts
          path: artifacts

      - name: üîΩ Pull image (if pushed)
        if: ${{ github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') }}
        run: |
          docker pull ${{ env.REGISTRY }}/${{ github.repository }}:${{ env.INFER_TAG }}

      - name: üìä Run container and compute latency metrics
        run: |
          IMAGE=${{ env.REGISTRY }}/${{ github.repository }}:${{ env.INFER_TAG }}
          # –Ø–∫—â–æ –æ–±—Ä–∞–∑ –Ω–µ —É —Ä–µ—î—Å—Ç—Ä—ñ (PR), —Å–ø—Ä–æ–±—É—î–º–æ –ª–æ–∫–∞–ª—å–Ω–∏–π —Ç–µ–≥
          if ! docker inspect "$IMAGE" > /dev/null 2>&1; then
            IMAGE=${{ env.REGISTRY }}/${{ github.repository }}:latest
          fi
          if ! docker inspect "$IMAGE" > /dev/null 2>&1; then
            echo "Falling back to locally built image"
            IMAGE=$(docker images --format '{{.Repository}}:{{.Tag}}' | grep '${{ env.IMAGE_NAME }}' | head -n1)
          fi
          mkdir -p metrics
          docker run --rm -v "$PWD/metrics:/metrics" "$IMAGE" bash -lc "python - <<'PY' \nimport json, csv, torch\nfrom pathlib import Path\nfrom model_utils import SmallCNN\nmodel = SmallCNN(n_classes=4)\nmodel.load_state_dict(torch.load('model_state_dict.pt', map_location='cpu'))\nmodel.eval()\nexample = torch.randn(1,1,64,32)\n# warmup\nfor _ in range(3):\n    _ = model(example)\nimport time\nN=30\nt=[]\nwith torch.no_grad():\n    for _ in range(N):\n        s=time.time(); _=model(example); e=time.time(); t.append((e-s)*1000)\nlat_ms=sum(t)/len(t)\nPath('/metrics/metrics.json').write_text(json.dumps({'latency_ms':lat_ms,'runs':N}, indent=2))\nwith open('/metrics/metrics.csv','w',newline='') as f:\n    w=csv.writer(f); w.writerow(['metric','value']); w.writerow(['latency_ms',lat_ms])\nprint('Latency (ms):',lat_ms)\nPY"

      - name: üì¶ Upload metrics artifacts
        uses: actions/upload-artifact@v4
        with:
          name: latency-metrics
          path: metrics

# –ü—Ä–∏–º—ñ—Ç–∫–∞: —â–æ–± –±–ª–æ–∫—É–≤–∞—Ç–∏ merge –≤ master/main –ø—Ä–∏ –ø–∞–¥—ñ–Ω–Ω—ñ –ø–∞–π–ø–ª–∞–π–Ω—É, –≤–≤—ñ–º–∫–Ω—ñ—Ç—å Branch protection rules —ñ –¥–æ–¥–∞–π—Ç–µ –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–π —Å—Ç–∞—Ç—É—Å —á–µ–∫ —Ü—å–æ–≥–æ workflow.

